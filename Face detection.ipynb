{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filter():\n",
    "    def __init__(self, model, transform):\n",
    "        self.landmark_detect = model\n",
    "        self.transform = transform\n",
    "        self.face_detect = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
    "        self.landmark_detect.eval()\n",
    "        \n",
    "    def landmarks_detect(self, image):\n",
    "        height, width, channels = image.shape\n",
    "        \n",
    "        image = self.transform(image=image)['image']\n",
    "        in_height, in_width, in_channel = image.shape\n",
    "        image = image.unsqueeze(0)\n",
    "        landmarks = self.landmark_detect(image)\n",
    "        landmarks = landmarks.detach().numpy()\n",
    "        landmarks = landmarks.reshape(68, 2)\n",
    "        \n",
    "        # convert to 0-224 axis\n",
    "        landmarks = (landmarks + 0.5) * in_width\n",
    "        # convert to original axis\n",
    "        landmarks[:, 0] = landmarks[:, 0] / in_width * width\n",
    "        landmarks[:, 1] = landmarks[:, 1] / in_width * height\n",
    "        \n",
    "        return landmarks\n",
    "        \n",
    "        \n",
    "    def full_landmarks_detect(self, image_path, radius=1):\n",
    "        # Load the image (np.ndarray)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Convert BGR to RGB (np.ndarray)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, channels = image_rgb.shape\n",
    "\n",
    "        # Detect faces in the image\n",
    "        faces = self.face_detect.process(image_rgb)\n",
    "\n",
    "        # Draw rectangles around the detected faces\n",
    "        for face in faces.detections:\n",
    "            bbox = face.location_data.relative_bounding_box\n",
    "            left = int(round(bbox.xmin * width))\n",
    "            top = int(round(bbox.ymin * height))\n",
    "            right = int(round((bbox.xmin + bbox.width) * width))\n",
    "            bottom = int(round((bbox.ymin + bbox.height) * height))\n",
    "            \n",
    "            landmarks = self.landmarks_detect(image_rgb[top:bottom, left:right])\n",
    "            landmarks = landmarks + [left, top]\n",
    "            \n",
    "            cv2.rectangle(image_rgb, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            for x, y in landmarks:\n",
    "                cv2.circle(image_rgb, (int(round(x)), int(round(y))), radius=radius, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "        # Display the image with detected faces\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pred = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dlib' has no attribute 'get_frontal_face_detector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frontal_face_detector\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dlib' has no attribute 'get_frontal_face_detector'"
     ]
    }
   ],
   "source": [
    "filter = Filter(model, transform_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/kaggle/input/test-input3/14.jpg'\n",
    "filter.full_landmarks_detect(image_path, radius=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
